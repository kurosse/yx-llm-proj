{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9613d5",
   "metadata": {},
   "source": [
    "### Generate ratings (chrF, BLEU, COMET)\n",
    "TODO: Present correlations as a table in paper for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af2413a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os, sys\n",
    "ROOT = os.path.abspath(\"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "\n",
    "from rich import print\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from src.utils.trad_metrics import TradMetrics\n",
    "from src.utils.altscore import get_altscores\n",
    "from lang_datasets.scripts.constants import LANGUAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25eb6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATINGS_FILE = \"../ratings/flores200-ratings.json\"\n",
    "TRANSLATIONS_FILE = \"../lang_datasets/sample_dataset/flores_sample_translations.json\"\n",
    "CANDIDATE_MAP = {\n",
    "    \"candidate_1\": \"google_translate\",\n",
    "    \"candidate_2\": \"nllb_translate\",\n",
    "    \"candidate_3\": \"llm_translate\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "trad_metrics = TradMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f3fae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RATINGS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    ratings = json.load(f)\n",
    "\n",
    "with open(TRANSLATIONS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    translations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5215b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per language model rating\n",
    "def get_candidate_ratings(candidate_id, languages):\n",
    "\n",
    "    candidate_ratings = {\n",
    "        candidate_id: {\n",
    "            language: {\n",
    "                \"sentence_bleu\": {\"scores\": [], \"avg\": None},\n",
    "                \"chrf\": {\"scores\": [], \"avg\": None},\n",
    "                \"comet\": {\"scores\": [], \"avg\": None},\n",
    "            }\n",
    "            for language in languages\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for data in tqdm(ratings):\n",
    "        curr_lang = data[\"language_code\"]\n",
    "        source_sentence = data[\"source_sentence\"]\n",
    "        reference_sentence = data[\"original_english_text\"]\n",
    "        for candidate in data[\"candidate_sentence_evaluations\"]:\n",
    "            if candidate[\"candidate_id\"] != candidate_id:\n",
    "                continue\n",
    "\n",
    "            candidate_sentence = candidate[\"candidate_sentence\"]\n",
    "            review_all_scores = trad_metrics.review_all_models(\n",
    "                [candidate_sentence], reference_sentence, source_sentence\n",
    "            )\n",
    "\n",
    "        # Append the scores to the candidate ratings\n",
    "        candidate_ratings[candidate_id][curr_lang][\"sentence_bleu\"][\"scores\"].append(\n",
    "            {\n",
    "                \"score\": review_all_scores[\"sentence_bleu\"][0],\n",
    "                \"translation\": candidate_sentence,\n",
    "            }\n",
    "        )\n",
    "        candidate_ratings[candidate_id][curr_lang][\"chrf\"][\"scores\"].append(\n",
    "            {\"score\": review_all_scores[\"chrf\"][0], \"translation\": candidate_sentence}\n",
    "        )\n",
    "        candidate_ratings[candidate_id][curr_lang][\"comet\"][\"scores\"].append(\n",
    "            {\"score\": review_all_scores[\"comet\"][0], \"translation\": candidate_sentence}\n",
    "        )\n",
    "\n",
    "        # Update the average scores\n",
    "        curr_bleu_score_sum = sum(\n",
    "            score[\"score\"]\n",
    "            for score in candidate_ratings[candidate_id][curr_lang][\"sentence_bleu\"][\n",
    "                \"scores\"\n",
    "            ]\n",
    "        )\n",
    "        curr_chrf_score_sum = sum(\n",
    "            score[\"score\"]\n",
    "            for score in candidate_ratings[candidate_id][curr_lang][\"chrf\"][\"scores\"]\n",
    "        )\n",
    "        curr_comet_score_sum = sum(\n",
    "            score[\"score\"]\n",
    "            for score in candidate_ratings[candidate_id][curr_lang][\"comet\"][\"scores\"]\n",
    "        )\n",
    "\n",
    "        candidate_ratings[candidate_id][curr_lang][\"sentence_bleu\"][\"avg\"] = (\n",
    "            curr_bleu_score_sum\n",
    "            / len(candidate_ratings[candidate_id][curr_lang][\"sentence_bleu\"][\"scores\"])\n",
    "        )\n",
    "        candidate_ratings[candidate_id][curr_lang][\"chrf\"][\"avg\"] = (\n",
    "            curr_chrf_score_sum\n",
    "            / len(candidate_ratings[candidate_id][curr_lang][\"chrf\"][\"scores\"])\n",
    "        )\n",
    "        candidate_ratings[candidate_id][curr_lang][\"comet\"][\"avg\"] = (\n",
    "            curr_comet_score_sum\n",
    "            / len(candidate_ratings[candidate_id][curr_lang][\"comet\"][\"scores\"])\n",
    "        )\n",
    "\n",
    "    return candidate_ratings\n",
    "\n",
    "def generate_ratings(cands_output_files, output_path): \n",
    "    for cands_output_file in cands_output_files:\n",
    "        file_output_path = os.path.join(output_path, cands_output_file)\n",
    "\n",
    "        # As long as the output file does not exist, we will write the ratings to the file\n",
    "        if not os.path.exists(file_output_path):\n",
    "\n",
    "            print(f\"{cands_output_file} does not exist, generating ratings for all...\")\n",
    "\n",
    "            google_trans_ratings = get_candidate_ratings(\"candidate_1\", LANGUAGES.keys())\n",
    "            nllb_trans_ratings = get_candidate_ratings(\"candidate_2\", LANGUAGES.keys()) \n",
    "            llm_trans_ratings = get_candidate_ratings(\"candidate_3\", LANGUAGES.keys())\n",
    "\n",
    "            with open(os.path.join(f\"{output_path}/google_ratings.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(google_trans_ratings, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "            with open(os.path.join(f\"{output_path}/nllb_ratings.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(nllb_trans_ratings, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "            with open(os.path.join(f\"{output_path}/llm_ratings.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(llm_trans_ratings, f, indent=4, ensure_ascii=False)\n",
    "            \n",
    "            return f\"Rating generation complete for chrF, BLEU, COMET scores. Saved at {output_path}.\"\n",
    "    \n",
    "    print(\"All ratings files already exist, skipping generation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dba1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"../ratings/\"\n",
    "cands_output_files = [\"google_ratings.json\", \"nllb_ratings.json\", \"llm_ratings.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e981ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">google_ratings.json does not exist, generating ratings for all<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "google_ratings.json does not exist, generating ratings for all\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 120/120 [02:12<00:00,  1.10s/it]\n",
      "100%|██████████| 120/120 [01:50<00:00,  1.09it/s]\n",
      "100%|██████████| 120/120 [03:06<00:00,  1.55s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Rating generation complete for chrF, BLEU, COMET scores. Saved at ../ratings/.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ratings(cands_output_files= cands_output_files, output_path=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f1aa2",
   "metadata": {},
   "source": [
    "### Generate ratings (ALTScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da5b52c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rating generation complete for ALTScore. Ratings saved to ..<span style=\"color: #800080; text-decoration-color: #800080\">/ratings//</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">alt_bench_ratings.json.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rating generation complete for ALTScore. Ratings saved to ..\u001b[35m/ratings/\u001b[0m\u001b[35m/\u001b[0m\u001b[95malt_bench_ratings.json.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratings_dict = {\n",
    "    f\"candidate_{i}\": {\n",
    "        lang: {\"scores\": [], \"avg_score\": None} for lang in LANGUAGES.keys()\n",
    "    }\n",
    "    for i in range(1, len(CANDIDATE_MAP) + 1)\n",
    "}\n",
    "\n",
    "for i, rating in enumerate(ratings):\n",
    "    try:\n",
    "        altscores = get_altscores(rating)\n",
    "        lang = rating[\"language_code\"]\n",
    "        for candidate, score in altscores.items():\n",
    "            ratings_dict[candidate][lang][\"scores\"].append(altscores[candidate])\n",
    "            curr_score = 0\n",
    "            for score in ratings_dict[candidate][lang][\"scores\"]:\n",
    "                curr_score += score[\"altscore\"]\n",
    "            ratings_dict[candidate][lang][\"avg_score\"] = curr_score / len(\n",
    "                ratings_dict[candidate][lang][\"scores\"]\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing rating {i}: {e}\")\n",
    "        print(rating)\n",
    "        break\n",
    "\n",
    "# Save ratings dict to a JSON file\n",
    "OUTPUT_PATH_ALT = f\"{OUTPUT_PATH}/alt_bench_ratings.json\"\n",
    "\n",
    "with open(OUTPUT_PATH_ALT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ratings_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Rating generation complete for ALTScore. Ratings saved to {OUTPUT_PATH_ALT}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a4dc0",
   "metadata": {},
   "source": [
    "### Computing Pearson and Spearman correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69a02b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"../ratings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89ed41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ratings\n",
    "google_ratings = json.load(open(os.path.join(OUTPUT_PATH, \"google_ratings.json\"), \"r\", encoding=\"utf-8\"))\n",
    "nllb_ratings = json.load(open(os.path.join(OUTPUT_PATH, \"nllb_ratings.json\"), \"r\", encoding=\"utf-8\"))\n",
    "llm_ratings = json.load(open(os.path.join(OUTPUT_PATH, \"llm_ratings.json\"), \"r\", encoding=\"utf-8\"))\n",
    "altscores = json.load(open(os.path.join(OUTPUT_PATH, \"alt_bench_ratings.json\"), \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6d43bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the altscores to the ratings\n",
    "def add_alt_scores(rating, altscores, candidate_id):\n",
    "    for lang, _ in rating[candidate_id].items():\n",
    "        rating[candidate_id][lang][\"altscore\"] = {\"scores\": []}\n",
    "        rating[candidate_id][lang][\"altscore\"][\"scores\"] = altscores[candidate_id][lang][\"scores\"]\n",
    "\n",
    "add_alt_scores(google_ratings, altscores, \"candidate_1\")\n",
    "add_alt_scores(nllb_ratings, altscores, \"candidate_2\")\n",
    "add_alt_scores(llm_ratings, altscores, \"candidate_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b1ec51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlations(ratings, candidate_id, lang):\n",
    "\n",
    "    # Ensure that the translations are in order\n",
    "    sentence_bleu_translations = [\n",
    "        score_dict[\"translation\"]\n",
    "        for score_dict in ratings[candidate_id][lang][\"sentence_bleu\"][\"scores\"]\n",
    "    ]\n",
    "    chrf_translations = [\n",
    "        score_dict[\"translation\"]\n",
    "        for score_dict in ratings[candidate_id][lang][\"chrf\"][\"scores\"]\n",
    "    ]\n",
    "    comet_translations = [\n",
    "        score_dict[\"translation\"]\n",
    "        for score_dict in ratings[candidate_id][lang][\"comet\"][\"scores\"]\n",
    "    ]\n",
    "    altscore_translations = [\n",
    "        score_dict[\"translation\"]\n",
    "        for score_dict in ratings[candidate_id][lang][\"altscore\"][\"scores\"]\n",
    "    ]\n",
    "\n",
    "    assert (\n",
    "        sentence_bleu_translations\n",
    "        == chrf_translations\n",
    "        == comet_translations\n",
    "        == altscore_translations\n",
    "    ), \"Translations for the different metrics do not match. Please check the ratings data.\"\n",
    "\n",
    "    # Extract the scores for the specified candidate and language\n",
    "    sentence_bleu_scores = [\n",
    "        score_dict[\"score\"]\n",
    "        for score_dict in ratings[candidate_id][lang][\"sentence_bleu\"][\"scores\"]\n",
    "    ]\n",
    "    chrf_scores = [\n",
    "        score_dict[\"score\"]\n",
    "        for score_dict in ratings[candidate_id][lang][\"chrf\"][\"scores\"]\n",
    "    ]\n",
    "    comet_scores = [\n",
    "        score_dict[\"score\"]\n",
    "        for score_dict in ratings[candidate_id][lang][\"comet\"][\"scores\"]\n",
    "    ]\n",
    "    altscore_scores = [\n",
    "        score_dict[\"altscore\"]\n",
    "        for score_dict in ratings[candidate_id][lang][\"altscore\"][\"scores\"]\n",
    "    ]\n",
    "\n",
    "    # Calculate Pearson and Spearman correlations\n",
    "    pearson_bleu_altscore = pearsonr(sentence_bleu_scores, altscore_scores)\n",
    "    spearman_bleu_altscore = spearmanr(sentence_bleu_scores, altscore_scores)\n",
    "\n",
    "    pearson_chrf_altscore = pearsonr(chrf_scores, altscore_scores)\n",
    "    spearman_chrf_altscore = spearmanr(chrf_scores, altscore_scores)\n",
    "\n",
    "    pearson_comet_altscore = pearsonr(comet_scores, altscore_scores)\n",
    "    spearman_comet_altscore = spearmanr(comet_scores, altscore_scores)\n",
    "\n",
    "    pearson_bleu_comet = pearsonr(sentence_bleu_scores, comet_scores)\n",
    "    spearman_bleu_comet = spearmanr(sentence_bleu_scores, comet_scores)\n",
    "\n",
    "    pearson_chrf_bleu = pearsonr(chrf_scores, sentence_bleu_scores)\n",
    "    spearman_chrf_bleu = spearmanr(chrf_scores, sentence_bleu_scores)\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"pearson_bleu_altscore\": pearson_bleu_altscore,\n",
    "            \"spearman_bleu_altscore\": spearman_bleu_altscore,\n",
    "            \"pearson_chrf_altscore\": pearson_chrf_altscore,\n",
    "            \"spearman_chrf_altscore\": spearman_chrf_altscore,\n",
    "            \"pearson_comet_altscore\": pearson_comet_altscore,\n",
    "            \"spearman_comet_altscore\": spearman_comet_altscore,\n",
    "            \"pearson_bleu_comet\": pearson_bleu_comet,\n",
    "            \"spearman_bleu_comet\": spearman_bleu_comet,\n",
    "            \"pearson_chrf_bleu\": pearson_chrf_bleu,\n",
    "            \"spearman_chrf_bleu\": spearman_chrf_bleu,\n",
    "            \"sentence_bleu_scores\": sentence_bleu_scores,\n",
    "            \"chrf_scores\": chrf_scores,\n",
    "            \"comet_scores\": comet_scores,\n",
    "            \"altscore_scores\": altscore_scores,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "google_corrs = (\n",
    "    compute_correlations(google_ratings, \"candidate_1\", \"tha_Thai\")\n",
    ")\n",
    "nllb_corrs = compute_correlations(nllb_ratings, \"candidate_2\", \"tha_Thai\")\n",
    "llm_corrs = compute_correlations(llm_ratings, \"candidate_3\", \"tha_Thai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50c8cebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Correlations saved to ..<span style=\"color: #800080; text-decoration-color: #800080\">/ratings//</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">correlations.json.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Correlations saved to ..\u001b[35m/ratings/\u001b[0m\u001b[35m/\u001b[0m\u001b[95mcorrelations.json.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "google_corrs = {}\n",
    "nllb_corrs = {}\n",
    "llm_corrs = {}\n",
    "\n",
    "for lang in LANGUAGES.keys():\n",
    "    google_corrs[lang] = compute_correlations(google_ratings, \"candidate_1\", lang)\n",
    "    nllb_corrs[lang] = compute_correlations(nllb_ratings, \"candidate_2\", lang)\n",
    "    llm_corrs[lang] = compute_correlations(llm_ratings, \"candidate_3\", lang)\n",
    "\n",
    "all_corrs = {\n",
    "    \"google\": google_corrs,\n",
    "    \"nllb\": nllb_corrs,\n",
    "    \"llm\": llm_corrs,\n",
    "}\n",
    "\n",
    "# Save the correlations to a JSON file\n",
    "CORRELATIONS_OUTPUT_PATH = f\"{OUTPUT_PATH}/correlations.json\"\n",
    "with open(CORRELATIONS_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_corrs, f, indent=4, ensure_ascii=False)\n",
    "print(f\"Correlations saved to {CORRELATIONS_OUTPUT_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4ad13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omscs-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
