{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a8a391a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2f2c7f361c4beb9616fbcb217e68cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../../../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/yixiantan/opt/miniconda3/envs/omscs-llm/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use evaluate for BLEU and chrf, use comet-babel for COMET\n",
    "import evaluate\n",
    "import sacrebleu\n",
    "import multiprocessing as mp\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "comet = evaluate.load(\"comet\")\n",
    "\n",
    "try:\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "except RuntimeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "640d3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_single_ref_score(model, predictions, reference, source=None):\n",
    "    \"\"\"\n",
    "    COMET requires the source text in the original language.\n",
    "    \"\"\"\n",
    "\n",
    "    scores = []\n",
    "    for prediction in predictions:\n",
    "\n",
    "        match model:\n",
    "            # case \"bleu\":\n",
    "            #     scores.append(bleu.compute(predictions=[prediction], references=[reference]))\n",
    "            case \"sentence_bleu\":\n",
    "                scores.append(sacrebleu.sentence_bleu(prediction, [reference]).score)\n",
    "            case \"chrf\":\n",
    "                scores.append(chrf.compute(predictions=[prediction], references=[reference])[\"score\"])\n",
    "            case \"comet\":\n",
    "                if source is None:\n",
    "                    raise ValueError(\"COMET requires source texts for scoring.\")\n",
    "                scores.append(comet.compute(predictions=[prediction], references=[reference], sources=[source], gpus=1, progress_bar=False)[\"scores\"][0])\n",
    "            case _:\n",
    "                raise ValueError(f\"Unknown model: {model}\")\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def review_all_models(predictions, reference, sources=None):\n",
    "    \"\"\"\n",
    "    Calculate scores for all predictions against their corresponding references.\n",
    "    If sources are provided, they will be used for COMET scoring.\n",
    "    \"\"\"\n",
    "    scores = {\n",
    "        \"sentence_bleu\": [],\n",
    "        \"chrf\": [],\n",
    "        \"comet\": []\n",
    "    }\n",
    "\n",
    "    for k, v in scores.items():\n",
    "        result = calculate_single_ref_score(k, predictions, reference, sources)\n",
    "        scores[k] = result\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2276b826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence BLEU score: [100.00000000000004, 66.87403049764218]\n",
      "CHRF score: [100.0, 93.2143290609026]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMET score: [0.9899783730506897, 0.8943490386009216]\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"this is a test\", \"this is a test too\"]\n",
    "reference = \"this is a test\"\n",
    "source = \"le test est un test\"\n",
    "\n",
    "# sentence_bleu\n",
    "scores = calculate_single_ref_score(\"sentence_bleu\", predictions, reference)\n",
    "print(f\"Sentence BLEU score: {scores}\")  # ➜ Sentence BLEU score: 100.0\n",
    "\n",
    "# chrf\n",
    "scores = calculate_single_ref_score(\"chrf\", predictions, reference)\n",
    "print(f\"CHRF score: {scores}\")  # ➜ CHRF score: 100.0\n",
    "\n",
    "# comet\n",
    "scores = calculate_single_ref_score(\"comet\", predictions, reference, source)\n",
    "print(f\"COMET score: {scores}\")  # ➜ COMET score: 0.123456 (example value, actual value will vary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f0bc2235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review all scores: {'sentence_bleu': [100.00000000000004], 'chrf': [100.0], 'comet': [0.9899783730506897]}\n"
     ]
    }
   ],
   "source": [
    "review_all_scores = review_all_models(predictions, reference, source)\n",
    "print(f\"Review all scores: {review_all_scores}\")  # ➜ Review all scores: [100.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3347331f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review all scores: {'sentence_bleu': [9.425159511373677, 36.28241434631104, 0.0], 'chrf': [61.89834169664148, 66.83394759294124, 1.3020833333333335], 'comet': [0.8905420899391174, 0.581606924533844, 0.34312549233436584]}\n"
     ]
    }
   ],
   "source": [
    "source = \"经理审查了报告，并提供了反馈。\"\n",
    "reference = \"The manager reviewed the report and provided feedback.\"\n",
    "predictions = [\"Feedback was provided by the manager after reviewing the report.\", \"The manager reviewed the cat and provided dog\", \"a1a2\"]\n",
    "\n",
    "review_all_scores = review_all_models(predictions, reference, source)\n",
    "print(f\"Review all scores: {review_all_scores}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omscs-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
