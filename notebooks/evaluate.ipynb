{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8a391a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2637f2ed778a436589e52d509f969d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../../../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/yixiantan/opt/miniconda3/envs/omscs-llm/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "\n",
    "import evaluate\n",
    "import sacrebleu\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "comet = evaluate.load(\"comet\")\n",
    "\n",
    "try:\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "logging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.ERROR)\n",
    "# For older Lightning versions, you may also need:\n",
    "logging.getLogger(\"pytorch_lightning.utilities.rank_zero\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640d3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_single_ref_score(model, predictions, reference, source=None):\n",
    "    \"\"\"\n",
    "    COMET requires the source text in the original language.\n",
    "    \"\"\"\n",
    "\n",
    "    scores = []\n",
    "    for prediction in predictions:\n",
    "\n",
    "        match model:\n",
    "            # case \"bleu\":\n",
    "            #     scores.append(bleu.compute(predictions=[prediction], references=[reference]))\n",
    "            case \"sentence_bleu\":\n",
    "                scores.append(sacrebleu.sentence_bleu(prediction, [reference]).score)\n",
    "            case \"chrf\":\n",
    "                scores.append(chrf.compute(predictions=[prediction], references=[reference])[\"score\"])\n",
    "            case \"comet\":\n",
    "                if source is None:\n",
    "                    raise ValueError(\"COMET requires source texts for scoring.\")\n",
    "                scores.append(comet.compute(predictions=[prediction], references=[reference], sources=[source], gpus=1, progress_bar=False)[\"scores\"][0])\n",
    "            case _:\n",
    "                raise ValueError(f\"Unknown model: {model}\")\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def review_all_models(predictions, reference, sources=None):\n",
    "    \"\"\"\n",
    "    Calculate scores for all predictions against their corresponding references.\n",
    "    If sources are provided, they will be used for COMET scoring.\n",
    "    \"\"\"\n",
    "    scores = {\n",
    "        \"sentence_bleu\": [],\n",
    "        \"chrf\": [],\n",
    "        \"comet\": []\n",
    "    }\n",
    "\n",
    "    for k, v in scores.items():\n",
    "        result = calculate_single_ref_score(k, predictions, reference, sources)\n",
    "        scores[k] = result\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2276b826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence BLEU score: [100.00000000000004, 66.87403049764218]\n",
      "CHRF score: [100.0, 93.2143290609026]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMET score: [0.9899783730506897, 0.8943490386009216]\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"this is a test\", \"this is a test too\"]\n",
    "reference = \"this is a test\"\n",
    "source = \"le test est un test\"\n",
    "\n",
    "# sentence_bleu\n",
    "scores = calculate_single_ref_score(\"sentence_bleu\", predictions, reference)\n",
    "print(f\"Sentence BLEU score: {scores}\")  # ➜ Sentence BLEU score: 100.0\n",
    "\n",
    "# chrf\n",
    "scores = calculate_single_ref_score(\"chrf\", predictions, reference)\n",
    "print(f\"CHRF score: {scores}\")  # ➜ CHRF score: 100.0\n",
    "\n",
    "# comet\n",
    "scores = calculate_single_ref_score(\"comet\", predictions, reference, source)\n",
    "print(f\"COMET score: {scores}\")  # ➜ COMET score: 0.123456 (example value, actual value will vary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0bc2235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review all scores: {'sentence_bleu': [100.00000000000004, 66.87403049764218], 'chrf': [100.0, 93.2143290609026], 'comet': [0.9899783730506897, 0.8943490386009216]}\n"
     ]
    }
   ],
   "source": [
    "review_all_scores = review_all_models(predictions, reference, source)\n",
    "print(f\"Review all scores: {review_all_scores}\")  # ➜ Review all scores: [100.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3347331f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review all scores: {'sentence_bleu': [9.980099403873663, 39.281465090051306, 26.084743001221455], 'chrf': [54.45980419390965, 84.94445636002766, 82.81454822707872], 'comet': [0.9057448506355286, 0.8854628801345825, 0.9285221695899963]}\n"
     ]
    }
   ],
   "source": [
    "source = \"经理审查了报告，并提供了反馈。\"\n",
    "reference = \"The manager reviewed the report and provided feedback.\"\n",
    "predictions = [\"After reviewing the report, the manager gave some feedback\", \"The report reviewed the manager and provided feedback.\", \"The manager review the report and provide feedback.\"]\n",
    "\n",
    "review_all_scores = review_all_models(predictions, reference, source)\n",
    "print(f\"Review all scores: {review_all_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "723f15e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review all scores: {'sentence_bleu': [38.03141958086991, 13.06511329838856], 'chrf': [55.24003630622099, 36.57487303249166], 'comet': [0.8867788910865784, 0.8811579942703247]}\n"
     ]
    }
   ],
   "source": [
    "source = \"部長におかれましては、報告書をご確認の上、貴重なご意見を賜りました。\"\n",
    "reference = \"The department head kindly reviewed the report and provided valuable feedback.\"\n",
    "predictions = [\"The manager reviewed the report and provided feedback.\", \"The head of department graciously looked over the report and shared constructive insights.\"]\n",
    "review_all_scores = review_all_models(predictions, reference, source)\n",
    "print(f\"Review all scores: {review_all_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de18dc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review all scores: {'sentence_bleu': [82.82477531331043, 60.04981752197521], 'chrf': [89.94918001993271, 80.94468790216838], 'comet': [0.9501563906669617, 0.9194379448890686]}\n"
     ]
    }
   ],
   "source": [
    "source = \"Saya pergi ke pasar untuk membeli beras, lalu menyiapkan semangkuk nasi yang lezat.\"\n",
    "reference = \"I went to the market to buy rice, and prepared a delicious bowl of rice.\"\n",
    "predictions = [\"I went to the market to buy rice, then prepared a delicious bowl of rice.\", \n",
    "               \"I went to the market to buy rice grains, then prepared a delicious bowl of cooked rice.\"]\n",
    "review_all_scores = review_all_models(predictions, reference, source)\n",
    "print(f\"Review all scores: {review_all_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe3294a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review all scores: {'sentence_bleu': [2.2869567780619007, 100.00000000000004], 'chrf': [8.743825924442664, 100.0], 'comet': [0.44467708468437195, 0.9866496920585632]}\n"
     ]
    }
   ],
   "source": [
    "source = \"Saya pergi ke pasar untuk membeli beras, lalu menyiapkan semangkuk nasi yang lezat.\"\n",
    "reference = \"I am dead\"\n",
    "predictions = [\"I went to the market to buy rice, then prepared a delicious bowl of rice.\", \n",
    "               \"I am dead\"]\n",
    "review_all_scores = review_all_models(predictions, reference, source)\n",
    "print(f\"Review all scores: {review_all_scores}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omscs-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
